# -*- coding: utf-8 -*-
# ì‹¤í–‰: streamlit run streamlit_app.py --server.port 8501 --server.address 0.0.0.0
import io
import os
from base64 import b64encode
import numpy as np
import pandas as pd
import requests
import streamlit as st

from pyecharts.charts import Line
from pyecharts import options as opts
from pyecharts.globals import ThemeType
from streamlit_echarts import st_pyecharts

# -------------------------------------------------
# ê¸°ë³¸ ì„¤ì • + í°íŠ¸ ì£¼ì…
# -------------------------------------------------
st.set_page_config(page_title="í•´ìˆ˜ë©´ ìƒìŠ¹ ëŒ€ì‹œë³´ë“œ (ECharts)", layout="wide", page_icon="ğŸŒŠ")


def inject_font_css(font_path="/fonts/Pretendard-Bold.ttf", family="Pretendard"):
    if not os.path.exists(font_path):
        return
    with open(font_path, "rb") as f:
        font_data = b64encode(f.read()).decode("utf-8")
    css = f"""
    <style>
    @font-face {{
        font-family: '{family}';
        src: url(data:font/ttf;base64,{font_data}) format('truetype');
        font-weight: 700; font-style: normal; font-display: swap;
    }}
    html, body, [class*="css"] {{
        font-family: '{family}', -apple-system, BlinkMacSystemFont, "Segoe UI",
                     Roboto, "Helvetica Neue", Arial, "Noto Sans KR", "Apple SD Gothic Neo",
                     "Nanum Gothic", "Malgun Gothic", sans-serif !important;
    }}
    </style>
    """
    st.markdown(css, unsafe_allow_html=True)


inject_font_css("/fonts/Pretendard-Bold.ttf", family="Pretendard")

# -------------------------------------------------
# í—¤ë”
# -------------------------------------------------
st.title("ğŸŒŠ ì „ì§€êµ¬ í‰ê·  í•´ìˆ˜ë©´(GMSL) â€” 1880 â†’ í˜„ì¬")
st.caption("ì¡°ìœ„ê³„ ì¬êµ¬ì„±(CSIRO) + ìœ„ì„± ê³ ë„ê³„(NOAA STAR) Â· 1880-01 ê°’ì„ 0ìœ¼ë¡œ ì¬ì •ë ¬ + 1993â€“2010 í‰ê· ì„  í‘œì‹œ")

with st.expander("â„¹ï¸ ë°ì´í„° ì„¤ëª… (CSIRO ì¥ê¸° ì‹œê³„ì—´ vs NOAA STAR ìœ„ì„± ì‹œê¸°)", expanded=False):
    st.markdown(
        """
**ì¥ê¸° ì‹œê³„ì—´ (1880â€“2009, CSIRO Tide-gauge Reconstruction)**  
- ì „ ì„¸ê³„ **ì—°ì•ˆ ì¡°ìœ„ê³„(tide gauge)** ê´€ì¸¡ì„ í†µê³„ì ìœ¼ë¡œ **ë³´ê°„Â·ì¬êµ¬ì„±**í•œ ì›”ë³„ **GMSL**ì…ë‹ˆë‹¤.  
- ì‹œê³„ì—´ì´ ê¸¸ì§€ë§Œ **ì—°ì•ˆ ì¤‘ì‹¬ ìƒ˜í”Œë§**, **GIA/IB ë“± ë³´ì • ê°€ì •**ì˜ ì˜í–¥ì´ ìˆìŠµë‹ˆë‹¤.

**ìœ„ì„± ì‹œê¸° (1993â€“í˜„ì¬, NOAA NESDIS/STAR Altimetry)**  
- TOPEX/Poseidon ì´í›„ **ìœ„ì„± ê³ ë„ê³„** ê¸°ë°˜ì˜ ì „ì§€êµ¬ í•´ìˆ˜ë©´ ë†’ì´ ìë£Œì…ë‹ˆë‹¤.  
- **ê¶¤ë„Â·ê³„ê¸° ë³´ì •, ì°¸ì¡°ë©´/ì§€ì˜¤ì´ë“œ ì²˜ë¦¬, ê³„ì ˆì„±(ì—°ì£¼ê¸°) ì œê±° ì—¬ë¶€** ë“±ì— ë”°ë¼ ê°’ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

**ì´ ëŒ€ì‹œë³´ë“œê°€ í•˜ëŠ” ì¼**  
- **ì‹œì‘ì—°ì›” ê°’ì„ 0 ê¸°ì¤€**ìœ¼ë¡œ ë¦¬ìŠ¤ì¼€ì¼í•´ ëˆ„ì  ë³€í™”ë¥¼ ì§ê´€ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.  
- **1993â€“2010 í‰ê· ì„ **ì„ í•¨ê»˜ ê·¸ë ¤ ë‘ ì†ŒìŠ¤ì˜ ë ˆë²¨ ì°¨ì´ë¥¼ íŒŒì•…í•˜ê¸° ì‰½ê²Œ í•©ë‹ˆë‹¤.
        """
    )

with st.expander("â“ 1993ë…„ ì „í›„ì— ë‹¨ì ˆì²˜ëŸ¼ ë³´ì´ëŠ” ì´ìœ ì™€ í•´ë²•", expanded=False):
    st.markdown(
        """
**ì°¨ì´ê°€ ì»¤ ë³´ì´ëŠ” ì£¼ëœ ì´ìœ **  
1) **ê¸°ì¤€ë©´(ë°ì´í…€)Â·ë³´ì • ì°¨ì´**: ì¡°ìœ„ê³„ ì¬êµ¬ì„±ê³¼ ìœ„ì„± ê³ ë„ê³„ì˜ ê¸°ì¤€ê³¼ ë³´ì • ì²´ê³„ê°€ ë‹¤ë¦…ë‹ˆë‹¤.  
2) **ê³„ì ˆì„± ì²˜ë¦¬ ì°¨ì´**: NOAAëŠ” **ì—°ì£¼ê¸° ìœ ì§€íŒ(keep)**ê³¼ **ì œê±°íŒ(free)** ë“± ì œí’ˆì— ë”°ë¼ í‰ê·  ë ˆë²¨ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
3) **ê³µê°„ ìƒ˜í”Œë§ ì°¨ì´**: ì¡°ìœ„ê³„ëŠ” **ì—°ì•ˆ ì¤‘ì‹¬**, ìœ„ì„±ì€ **ì „ì§€êµ¬ í•´ì–‘** ìƒ˜í”Œë§ì…ë‹ˆë‹¤.  
4) **ì¬êµ¬ì„±/í’ˆì§ˆê´€ë¦¬ ë°©ë²• ì°¨ì´**: í†µê³„ì  ë³´ê°„Â·ê°€ì¤‘Â·ê¸°ì¤€êµ¬ê°„ ì„ íƒ ë“± íŒŒì´í”„ë¼ì¸ ì°¨ì´.

**ì‹¤ë¬´ì  í•´ë²•(ë³¸ ëŒ€ì‹œë³´ë“œ)**  
- **ê²¹ì¹¨êµ¬ê°„(1993â€“2010) í‰ê· ì„ ì¼ì¹˜**ì‹œí‚¤ëŠ” ì˜¤í”„ì…‹ ë³´ì • ì˜µì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.  
- **12ê°œì›” ì´ë™í‰ê· **ì„ ê³µí†µ ì ìš©í•´ ê³„ì ˆì„± ì°¨ì´ë¥¼ ì™„í™”í•©ë‹ˆë‹¤.  
- NOAAì˜ **keep/free** ì¤‘ ê°€ìš© ì†ŒìŠ¤ë¥¼ ìë™ ì„ íƒí•©ë‹ˆë‹¤.
        """
    )

# -------------------------------------------------
# ì›ê²© CSV ë¡œë” & íŒŒì„œ
# -------------------------------------------------

def fetch_csv_from_candidates(candidates, **read_csv_kwargs) -> pd.DataFrame:
    last_err = None
    for url in candidates:
        try:
            r = requests.get(url, timeout=25)
            r.raise_for_status()
            df = (
                pd.read_csv(io.BytesIO(r.content), **read_csv_kwargs)
                if read_csv_kwargs
                else pd.read_csv(io.BytesIO(r.content))
            )
            df["__source_url__"] = url
            return df
        except Exception as e:
            last_err = e
    if last_err:
        raise last_err
    raise RuntimeError("No URL candidates provided")


def read_noaa_altimetry_csv(url: str) -> pd.DataFrame:
    """
    NOAA STAR CSV(ì£¼ì„/ê°€ë³€í¬ë§·) â†’ í‘œì¤€í™”:
      - ë‚ ì§œ: (year, month) ë˜ëŠ” (decimal year) ë˜ëŠ” ë¬¸ìì—´ ë‚ ì§œ ìë™ ì²˜ë¦¬
      - ê°’ì—´: ë’¤ìª½ ì—´ë¶€í„° ìˆ«ì ë¹„ìœ¨ ë†’ì€ ì—´ ì„ íƒ
      - NaN/NaT ì•ˆì „ ê°€ë“œ (ì •ìˆ˜ ë³€í™˜ì€ dropna ì´í›„ì—ë§Œ!)
    """
    r = requests.get(url, timeout=25)
    r.raise_for_status()
    txt = r.text

    # 1) ì£¼ì„/ë¹ˆì¤„ ì œê±°
    lines = [ln for ln in txt.splitlines() if ln.strip() and not ln.lstrip().startswith("#")]
    if not lines:
        raise ValueError("NOAA CSV: ìœ íš¨í•œ ë°ì´í„° ì¤„ì´ ì—†ìŠµë‹ˆë‹¤.")
    raw = "\n".join(lines)

    # 2) ìœ ì—° íŒŒì‹±
    df = (
        pd.read_csv(
            io.StringIO(raw),
            sep=r"[,\s]+",
            engine="python",
            header=None,
            comment="#",
            skip_blank_lines=True,
        )
        .dropna(axis=1, how="all")
    )

    if df.shape[1] < 2:
        raise ValueError("NOAA CSV: ì—´ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    c0 = df.iloc[:, 0]
    c1 = df.iloc[:, 1] if df.shape[1] > 1 else None

    def decyear_to_datetime(y):
        try:
            if pd.isna(y):
                return pd.NaT
            y = float(y)
            if not np.isfinite(y):
                return pd.NaT
        except Exception:
            return pd.NaT
        year = int(np.floor(y))
        rem = y - year
        return pd.Timestamp(year, 1, 1) + pd.to_timedelta(rem * 365.25, unit="D")

    d = None

    # (A) year, month íŒ¨í„´
    c0_num = pd.to_numeric(c0, errors="coerce")
    c1_num = pd.to_numeric(c1, errors="coerce") if c1 is not None else None
    if c1_num is not None:
        y_ok = c0_num.between(1800, 2100)
        m_ok = c1_num.between(1, 12)
        mask = y_ok & m_ok & c0_num.notna() & c1_num.notna()
        frac = mask.mean()
        if frac > 0.9:
            idx = mask[mask].index
            years = c0_num.loc[idx].astype("int64")
            months = c1_num.loc[idx].astype("int64")
            d_conv = pd.to_datetime(
                dict(year=years, month=months, day=np.ones(len(idx), dtype="int64")),
                errors="coerce",
            )
            d_full = pd.Series(pd.NaT, index=df.index)
            d_full.loc[idx] = d_conv
            d = d_full

    # (B) ì†Œìˆ˜ ì—°ë„
    if d is None:
        if c0_num.notna().mean() > 0.9 and c0_num.between(1800, 2100).mean() > 0.9:
            d = c0_num.map(decyear_to_datetime)

    # (C) ë¬¸ìì—´ ë‚ ì§œ
    if d is None:
        d_try = pd.to_datetime(c0, errors="coerce")
        if d_try.notna().mean() > 0.9:
            d = d_try

    if d is None:
        raise ValueError("NOAA CSV: ë‚ ì§œ ì—´ì„ í•´ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ê°’ ì—´: ë’¤ì—ì„œ ì•ìœ¼ë¡œ ìˆ«ì ë¹„ìœ¨ ë†’ì€ ì—´ ì‚¬ìš©
    valcol = None
    for col in reversed(range(1, df.shape[1])):
        col_vals = pd.to_numeric(df.iloc[:, col], errors="coerce")
        if col_vals.notna().mean() > 0.7:
            valcol = col
            break
    if valcol is None:
        raise ValueError("NOAA CSV ê°’ ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    out = (
        pd.DataFrame(
            {
                "date": pd.to_datetime(d, errors="coerce"),
                "gmsl_mm": pd.to_numeric(df.iloc[:, valcol], errors="coerce"),
            }
        )
        .dropna(subset=["date", "gmsl_mm"])  # ì™„ì „ ê²°ì¸¡ ì œê±°
        .sort_values("date")
    )

    # ì˜¤ëŠ˜(ì„œìš¸) ì´í›„ ë°ì´í„° ì»·
    today_seoul = pd.Timestamp.now(tz="Asia/Seoul").normalize().tz_localize(None)
    out = out[out["date"] <= today_seoul]

    out["__source_url__"] = url
    return out


# -------------------------------------------------
# ë°ì´í„° ì†ŒìŠ¤ URL
# -------------------------------------------------
CSIRO_CANDIDATES = [
    # DataHub: CSIRO ì¬êµ¬ì„±(1880â€“2009) ì›”ë³„ (Time, GMSL, GMSL uncertainty)
    "https://datahub.io/core/sea-level-rise/r/csiro_recons_gmsl_mo_2015.csv",
]

ALTIMETRY_CANDIDATES = [
    # NOAA STAR ìµœì‹ (ì—°ì£¼ê¸° ìœ ì§€)
    "https://www.star.nesdis.noaa.gov/socd/lsa/SeaLevelRise/slr/slr_sla_gbl_keep_ref_90.csv",
    # NOAA STAR ìµœì‹ (ì—°ì£¼ê¸° ì œê±°)
    "https://www.star.nesdis.noaa.gov/socd/lsa/SeaLevelRise/slr/slr_sla_gbl_free_all_66.csv",
]


# -------------------------------------------------
# ë°ëª¨ ë°±ì—… (ì˜¤ëŠ˜ ê¸°ì¤€ìœ¼ë¡œ ì»·)
# -------------------------------------------------

def demo_fallback() -> tuple[pd.DataFrame, pd.DataFrame]:
    # ì˜¤ëŠ˜(ì„œìš¸) ê¸°ì¤€ê¹Œì§€ë§Œ ìƒì„±
    today_seoul = pd.Timestamp.now(tz="Asia/Seoul").normalize().tz_localize(None)

    # 1) CSIRO êµ¬ê°„: 1900-01 ~ 1992-12 (ë‹¨, ë¯¸ë˜ë©´ ì˜¤ëŠ˜ë¡œ ì»·)
    end_a = min(today_seoul, pd.Timestamp("1992-12-01"))
    dates_a = pd.date_range("1900-01-01", end_a, freq="MS")
    mm_a = np.linspace(0, 120, len(dates_a)) + np.random.normal(0, 2, len(dates_a))
    df_a = pd.DataFrame({"date": dates_a, "gmsl_mm": mm_a, "__source_url__": "DEMO: CSIRO-like"})

    # 2) ìœ„ì„± êµ¬ê°„: 1993-01 ~ ì˜¤ëŠ˜(ì„œìš¸)
    start_b = pd.Timestamp("1993-01-01")
    end_b = min(today_seoul, pd.Timestamp("2025-12-01"))
    if end_b < start_b:
        df_b = pd.DataFrame(columns=["date", "gmsl_mm", "__source_url__"])
    else:
        dates_b = pd.date_range(start_b, end_b, freq="MS")
        last_base = float(mm_a[-1]) if len(mm_a) else 0.0
        mm_b = last_base + np.linspace(0, 110, len(dates_b)) + np.random.normal(0, 2, len(dates_b))
        df_b = pd.DataFrame({"date": dates_b, "gmsl_mm": mm_b, "__source_url__": "DEMO: NOAA-like"})

    return df_a, df_b


# -------------------------------------------------
# ë¡œë“œ & ê²°í•©
# -------------------------------------------------

@st.cache_data(show_spinner=True, ttl=60 * 30)
def load_sources():
    c, n = None, None

    # CSIRO ì¬êµ¬ì„±(ì›”ë³„)
    try:
        c = fetch_csv_from_candidates(
            CSIRO_CANDIDATES,
            usecols=["Time", "GMSL"],  # DataHub ìŠ¤í‚¤ë§ˆ
            parse_dates=["Time"],
        )
        c = c.rename(columns={"Time": "date", "GMSL": "gmsl_mm"})
        c = c[["date", "gmsl_mm", "__source_url__"]].dropna().sort_values("date")
    except Exception as e:
        st.warning(f"CSIRO ì‹¤ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ â†’ ë°ëª¨ ëŒ€ì²´: {e}")
        c = None

    # ìœ„ì„±(altimetry) â€” NOAA STAR 2ì¢… ì¤‘ ì„±ê³µí•˜ëŠ” ê²ƒ ì‚¬ìš©
    try:
        last_err = None
        n = None
        for url in ALTIMETRY_CANDIDATES:
            try:
                n = read_noaa_altimetry_csv(url)
                break
            except Exception as e:
                last_err = e
        if n is None:
            raise last_err if last_err else RuntimeError("NOAA candidates failed")
    except Exception as e:
        st.warning(f"ìœ„ì„± ì‹¤ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ â†’ ë°ëª¨ ëŒ€ì²´: {e}")
        n = None

    # Fallback
    if c is None or n is None:
        demo_c, demo_n = demo_fallback()
        c = demo_c if c is None else c
        n = demo_n if n is None else n

    return c, n


def unify_concat(df_a, df_b):
    """1993ë…„ ì´í›„ëŠ” ìœ„ì„±ê°’ ìš°ì„ ìœ¼ë¡œ ê²°í•©"""
    a = df_a.copy()
    b = df_b.copy()
    a["date"] = pd.to_datetime(a["date"], errors="coerce")
    b["date"] = pd.to_datetime(b["date"], errors="coerce")
    a = a.dropna(subset=["date"])  # ì•ˆì „ ê°€ë“œ
    b = b.dropna(subset=["date"])  # ì•ˆì „ ê°€ë“œ

    a["src"] = "ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)"
    b["src"] = "ìœ„ì„± ê³ ë„ê³„ (NOAA)"
    if not b.empty:
        start_b = b["date"].min()
        a = a[a["date"] < start_b]
    return pd.concat([a, b], ignore_index=True).sort_values("date")


# ì‹¤ì œ ë¡œë“œ

df_csi, df_noaa = load_sources()
full = unify_concat(df_csi, df_noaa)

# ê²°í•© í›„ ë¯¸ë˜ ì»· (ì„œìš¸ ê¸°ì¤€)
_cutoff = pd.Timestamp.now(tz="Asia/Seoul").normalize().tz_localize(None)
full = full[pd.to_datetime(full["date"], errors="coerce") <= _cutoff]

# -------------------------------------------------
# ì‚¬ì´ë“œë°”(í•œêµ­ì–´)
# -------------------------------------------------
st.sidebar.header("âš™ï¸ ì„¤ì •")
theme = ThemeType.LIGHT  # ê³ ì •(í•„ìš”í•˜ë©´ í† ê¸€ ê°€ëŠ¥)
unit = st.sidebar.radio("ë‹¨ìœ„", ["mm", "inch"], index=0, horizontal=True)
smooth = st.sidebar.checkbox("12ê°œì›” ì´ë™í‰ê· (ìŠ¤ë¬´ë”©)", value=True)
show_markers = st.sidebar.checkbox("í¬ì¸íŠ¸ í‘œì‹œ", value=False)
st.sidebar.markdown("---")
apply_offset = st.sidebar.checkbox("1993â€“2010 í‰ê· ìœ¼ë¡œ ì†ŒìŠ¤ ì˜¤í”„ì…‹ ë³´ì •", value=False)

_dates = pd.to_datetime(full["date"], errors="coerce").dropna()
if _dates.empty:
    st.error("ìœ íš¨í•œ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ì†ŒìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()
min_year = int(_dates.min().year)
max_year = int(_dates.max().year)

default_start = max(1880, min_year)
default_end = min(2025, max_year)

st.sidebar.markdown("---")
year_range = st.sidebar.slider(
    "í‘œì‹œ ê¸°ê°„(ì—°ë„)", min_value=min_year, max_value=max(2025, max_year), value=(default_start, default_end), step=1
)

with st.sidebar.expander("ë°ì´í„° ì†ŒìŠ¤ ìƒíƒœ", expanded=False):
    try:
        st.write("CSIRO:", df_csi["__source_url__"].iloc[0])
    except Exception:
        st.write("CSIRO: (ë°ëª¨)")
    try:
        st.write("NOAA :", df_noaa["__source_url__"].iloc[0])
        st.write("NOAA ìµœì‹  ì‹œì :", pd.to_datetime(df_noaa["date"]).max().date())
    except Exception:
        st.write("NOAA : (ë°ëª¨)")

# -------------------------------------------------
# ê°€ê³µ (ìŠ¤ë¬´ë”©/ë‹¨ìœ„ â†’ 1880-01 ê¸°ì¤€ 0 ì¬ì •ë ¬ + 1993â€“2010 í‰ê· ì„ )
# -------------------------------------------------
plot_df = full.copy()
plot_df["year"] = pd.to_datetime(plot_df["date"], errors="coerce").dt.year
plot_df = plot_df[(plot_df["year"] >= year_range[0]) & (plot_df["year"] <= year_range[1])]
plot_df = plot_df.sort_values("date")

# NaT ì œê±°
_plot_dates = pd.to_datetime(plot_df["date"], errors="coerce")
plot_df = plot_df.loc[_plot_dates.notna()].copy()
plot_df["date"] = _plot_dates.loc[_plot_dates.notna()]

# ìŠ¤ë¬´ë”©
if smooth:
    plot_df["gmsl_mm_smooth"] = plot_df.groupby("src")["gmsl_mm"].transform(lambda s: s.rolling(12, min_periods=1).mean())
    value_col = "gmsl_mm_smooth"
else:
    value_col = "gmsl_mm"

# ë‹¨ìœ„ ë³€í™˜
if unit == "inch":
    plot_df["value"] = plot_df[value_col] / 25.4
    unit_label = "in"
    unit_label_ko = "ì¸ì¹˜"
else:
    plot_df["value"] = plot_df[value_col]
    unit_label = "mm"
    unit_label_ko = "mm"

# === 1880-01 ê¸°ì¤€ 0 ì¬ì •ë ¬ ===
baseline_date = pd.Timestamp(1880, 1, 1)
if baseline_date in list(plot_df["date"]):
    baseline_val = plot_df.loc[plot_df["date"] == baseline_date, "value"].mean()
else:
    first_year = int(plot_df["date"].dt.year.min())
    baseline_val = plot_df.loc[plot_df["date"].dt.year == first_year, "value"].mean()
plot_df["value_adj"] = plot_df["value"] - baseline_val

# --- 1993â€“2010 ê²¹ì¹¨êµ¬ê°„ í‰ê· ìœ¼ë¡œ ì˜¤í”„ì…‹ ë³´ì •(ì„ íƒ) ---
if apply_offset:
    overlap_mask = (plot_df["date"] >= pd.Timestamp(1993, 1, 1)) & (plot_df["date"] <= pd.Timestamp(2010, 12, 31))
    over = plot_df.loc[overlap_mask].copy()
    if not over.empty and over["src"].nunique() >= 2:
        src_mean = over.groupby("src")["value_adj"].mean()
        need = {"ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)", "ìœ„ì„± ê³ ë„ê³„ (NOAA)"}
        if need.issubset(set(src_mean.index)):
            offset = float(src_mean["ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)"] - src_mean["ìœ„ì„± ê³ ë„ê³„ (NOAA)"])
            plot_df.loc[plot_df["src"] == "ìœ„ì„± ê³ ë„ê³„ (NOAA)", "value_adj"] += offset
            st.caption(f"ğŸ”§ ì˜¤í”„ì…‹ ë³´ì • ì ìš©: NOAA ì‹œë¦¬ì¦ˆì— {offset:.2f} {unit_label_ko}ë¥¼ ë”í•´ CSIRO(1993â€“2010 í‰ê· )ì— ì •ë ¬í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.caption("âš ï¸ ì˜¤í”„ì…‹ ë³´ì • ë¶ˆê°€: 1993â€“2010 ê²¹ì¹¨êµ¬ê°„ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# === 1993â€“2010 êµ¬ê°„ í‰ê· (ìˆ˜í‰ì„  ìš©) ===
win_start = pd.Timestamp(1993, 1, 1)
win_end = pd.Timestamp(2010, 12, 31)
mask_9310 = (plot_df["date"] >= win_start) & (plot_df["date"] <= win_end)
avg_9310 = float(plot_df.loc[mask_9310, "value_adj"].mean()) if mask_9310.any() else float(plot_df["value_adj"].mean())

# -------------------------------------------------
# í†µê³„: ë³€í™”ëŸ‰/ì—°í‰ê·  ìƒìŠ¹ë¥  (1880-01=0 ë³´ì • í›„)
# -------------------------------------------------

def annual_rate(df):
    s = df.sort_values("date")["value_adj"].dropna()
    d = df.sort_values("date")["date"].dropna()
    if len(s) < 2:
        return np.nan, np.nan
    change = s.iloc[-1] - s.iloc[0]
    years = (d.iloc[-1] - d.iloc[0]).days / 365.25
    if years <= 0:
        return np.nan, np.nan
    return change, change / years


change, rate = annual_rate(plot_df)

# -------------------------------------------------
# ì¶”ì„¸ì„ (ì„ í˜•íšŒê·€) â€” 1880-01=0 ë³´ì • í›„ ê°’ìœ¼ë¡œ ê³„ì‚°
# -------------------------------------------------

def trend_series(df):
    g = df[["date", "value_adj"]].dropna().sort_values("date")
    if len(g) < 2:
        return pd.DataFrame(columns=["date", "trend"]), np.nan
    year_float = g["date"].dt.year + (g["date"].dt.dayofyear - 1) / 365.25
    p = np.polyfit(year_float.values, g["value_adj"].values, 1)  # slope, intercept
    trend_vals = np.polyval(p, year_float.values)
    return pd.DataFrame({"date": g["date"].values, "trend": trend_vals}), p[0]


trend_df, slope_per_year = trend_series(plot_df)

# -------------------------------------------------
# ECharts ë¹Œë” (KOR ë¼ë²¨ + í°íŠ¸ + íˆ´ë°•ìŠ¤ + ê¸°ì¤€ì„  + 1993â€“2010 í‰ê· ì„ )
# -------------------------------------------------

def build_line_chart(df, trend_df, theme, unit_label, unit_label_ko, avg_9310, show_markers=False):
    # NaT ê°€ë“œ
    _d = pd.to_datetime(df["date"], errors="coerce").dropna()
    if _d.empty:
        empty = Line(init_opts=opts.InitOpts(theme=theme, width="1000px", height="560px"))
        empty.add_xaxis([])
        return empty
    start, end = _d.min(), _d.max()
    if pd.isna(start) or pd.isna(end) or start > end:
        empty = Line(init_opts=opts.InitOpts(theme=theme, width="1000px", height="560px"))
        empty.add_xaxis([])
        return empty

    x = pd.period_range(start, end, freq="M").strftime("%Y-%m").tolist()

    # âœ… ê³ ì • íŒ”ë ˆíŠ¸ (ì˜ë„í•œ ë²”ë¡€ ìƒ‰)
    color_map = {
        "ìœ„ì„± ê³ ë„ê³„ (NOAA)":   "#2563eb",  # íŒŒë‘
        "ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)": "#f97316",  # ì£¼í™©
        "ì¶”ì„¸ì„ ":               "#10b981",  # ì´ˆë¡
        "1993â€“2010 í‰ê· ":       "#facc15",  # ë…¸ë‘
    }
    palette_order = ["ìœ„ì„± ê³ ë„ê³„ (NOAA)", "ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)", "ì¶”ì„¸ì„ ", "1993â€“2010 í‰ê· "]

    chart = Line(init_opts=opts.InitOpts(theme=theme, width="1000px", height="560px"))
    chart.add_xaxis(xaxis_data=x)

    # âŠ ì‹œë¦¬ì¦ˆ ì¶”ê°€ ìˆœì„œ ê°•ì œ: NOAA â†’ CSIRO (ì¡´ì¬í•  ë•Œë§Œ)
    grouped = {k: v.sort_values("date").copy() for k, v in df.groupby("src")}
    for name in ["ìœ„ì„± ê³ ë„ê³„ (NOAA)", "ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)"]:
        if name not in grouped:
            continue
        g = grouped[name]
        g["xkey"] = g["date"].dt.strftime("%Y-%m")
        m = dict(zip(g["xkey"], g["value_adj"].round(2)))
        y_full = [m.get(xx, None) for xx in x]
        chart.add_yaxis(
            series_name=name,
            y_axis=y_full,
            is_smooth=True,
            symbol="circle",
            symbol_size=4,
            is_symbol_show=show_markers,
            is_connect_nones=False,
            # âŒ ì•„ì´í…œ/ë¼ì¸/ì—ì–´ë¦¬ì–´ ìƒ‰ ì „ë¶€ ëª…ì‹œ
            itemstyle_opts=opts.ItemStyleOpts(color=color_map[name], border_color=color_map[name]),
            linestyle_opts=opts.LineStyleOpts(width=3, opacity=0.95, color=color_map[name]),
            areastyle_opts=opts.AreaStyleOpts(opacity=0.18, color=color_map[name]),
        )

    # ì¶”ì„¸ì„ (ì´ˆë¡ ê³ ì •)
    if trend_df is not None and not trend_df.empty:
        tdf = trend_df.sort_values("date").copy()
        tdf["xkey"] = tdf["date"].dt.strftime("%Y-%m")
        tmap = dict(zip(tdf["xkey"], tdf["trend"].round(2)))
        y_tr_full = [tmap.get(xx, None) for xx in x]
        chart.add_yaxis(
            series_name="ì¶”ì„¸ì„ ",
            y_axis=y_tr_full,
            is_smooth=False,
            symbol="none",
            is_connect_nones=False,
            itemstyle_opts=opts.ItemStyleOpts(color=color_map["ì¶”ì„¸ì„ "], border_color=color_map["ì¶”ì„¸ì„ "]),
            linestyle_opts=opts.LineStyleOpts(width=2, type_="dashed", color=color_map["ì¶”ì„¸ì„ "]),
            areastyle_opts=opts.AreaStyleOpts(opacity=0),
        )

    # 1993â€“2010 í‰ê· ì„  (markLine ì „ìš© ì‹œë¦¬ì¦ˆ, ë…¸ë‘ ê³ ì •)
    chart.add_yaxis(
        series_name="1993â€“2010 í‰ê· ",
        y_axis=[None] * len(x),
        is_smooth=False,
        symbol="none",
        itemstyle_opts=opts.ItemStyleOpts(color=color_map["1993â€“2010 í‰ê· "], border_color=color_map["1993â€“2010 í‰ê· "]),
        linestyle_opts=opts.LineStyleOpts(width=0, opacity=0, color=color_map["1993â€“2010 í‰ê· "]),
        areastyle_opts=opts.AreaStyleOpts(opacity=0),
        markline_opts=opts.MarkLineOpts(
            data=[
                opts.MarkLineItem(y=0, name="ê¸°ì¤€ì„  (1880-01=0)"),
                opts.MarkLineItem(y=avg_9310, name="1993â€“2010 í‰ê· "),
            ],
            linestyle_opts=opts.LineStyleOpts(type_="dashed", opacity=0.6, color=color_map["1993â€“2010 í‰ê· "]),
            label_opts=opts.LabelOpts(font_family="Pretendard"),
        ),
    )

    # â‹ ê¸€ë¡œë²Œ íŒ”ë ˆíŠ¸ë„ ë™ì¼ ìˆœì„œë¡œ ê³ ì • (ë²”ë¡€ ì•„ì´ì½˜ ìƒ‰ ë³´ì •)
    chart.set_colors([color_map[k] for k in palette_order])

    chart.set_global_opts(
        legend_opts=opts.LegendOpts(
            pos_top="2%", pos_left="center",
            textstyle_opts=opts.TextStyleOpts(font_family="Pretendard"),
        ),
        tooltip_opts=opts.TooltipOpts(
            trigger="axis", axis_pointer_type="cross",
            value_formatter=f"{{value}} {unit_label_ko}",
        ),
        datazoom_opts=[opts.DataZoomOpts(type_="slider", range_start=0, range_end=100),
                       opts.DataZoomOpts(type_="inside")],
        xaxis_opts=opts.AxisOpts(
            type_="category", boundary_gap=False,
            axislabel_opts=opts.LabelOpts(font_family="Pretendard"),
            name_textstyle_opts=opts.TextStyleOpts(font_family="Pretendard"),
        ),
        yaxis_opts=opts.AxisOpts(
            name=f"ëˆ„ì  í•´ìˆ˜ë©´ ë³€í™” (ì‹œì‘ì—°ì›”=0, {unit_label_ko})",
            splitline_opts=opts.SplitLineOpts(is_show=True),
            axislabel_opts=opts.LabelOpts(font_family="Pretendard"),
            name_textstyle_opts=opts.TextStyleOpts(font_family="Pretendard"),
        ),
        toolbox_opts=opts.ToolboxOpts(
            is_show=True,
            feature=opts.ToolBoxFeatureOpts(
                save_as_image=opts.ToolBoxFeatureSaveAsImageOpts(title="PNG ì €ì¥"),
                restore=opts.ToolBoxFeatureRestoreOpts(),
                data_view=opts.ToolBoxFeatureDataViewOpts(is_show=False),
            ),
        ),
    )
    return chart



chart = build_line_chart(
    plot_df[["date", "src", "value_adj"]],
    trend_df,
    theme,
    unit_label,
    unit_label_ko,
    avg_9310=avg_9310,
    show_markers=show_markers,
)

st_pyecharts(
    chart,
    height="560px",
    key=f"echarts-{unit}-{year_range}-{smooth}-{show_markers}-{apply_offset}",
)

# -------------------------------------------------
# 1993â€“2010 í‰ê·  ë ˆë²¨ ë¹„êµ ìˆ«ì ë¦¬í¬íŠ¸(ì„ íƒ)
# -------------------------------------------------
with st.expander("ğŸ“ 1993â€“2010 ê²¹ì¹¨êµ¬ê°„ í‰ê·  ë ˆë²¨ ë¹„êµ(1880-01=0 ê¸°ì¤€)", expanded=False):
    over = plot_df[(plot_df["date"] >= pd.Timestamp(1993, 1, 1)) & (plot_df["date"] <= pd.Timestamp(2010, 12, 31))]
    if not over.empty and over["src"].nunique() >= 2:
        stats = over.groupby("src")["value_adj"].mean().rename("mean_value_adj").to_frame()
        try:
            csiro_mean = float(stats.loc["ì¡°ìœ„ê³„ ì¬êµ¬ì„± (CSIRO)", "mean_value_adj"])
            noaa_mean = float(stats.loc["ìœ„ì„± ê³ ë„ê³„ (NOAA)", "mean_value_adj"])
            diff = noaa_mean - csiro_mean
            st.write(f"- CSIRO í‰ê· : {csiro_mean:.2f} {unit_label_ko}")
            st.write(f"- NOAA í‰ê·  : {noaa_mean:.2f} {unit_label_ko}")
            st.write(f"- ì°¨ì´(=NOAAâˆ’CSIRO): **{diff:+.2f} {unit_label_ko}**")
        except Exception:
            st.write(stats)
    else:
        st.write("ê²¹ì¹¨êµ¬ê°„ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ë¹„êµê°€ ì–´ë µìŠµë‹ˆë‹¤.")

# -------------------------------------------------
# ë©”íŠ¸ë¦­ ì¹´ë“œ + í‘œ
# -------------------------------------------------
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("ê¸°ê°„", f"{year_range[0]}â€“{year_range[1]}")
with col2:
    st.metric(f"ë³€í™”ëŸ‰ ({unit_label_ko})", f"{(0 if np.isnan(change) else change):.1f}")
with col3:
    st.metric(f"ì—°í‰ê·  ìƒìŠ¹ë¥  ({unit_label_ko}/ë…„)", f"{(0 if np.isnan(rate) else rate):.2f}")
with col4:
    st.metric("ë°ì´í„° í¬ì¸íŠ¸", f"{len(plot_df):,}")

with st.expander("ğŸ§¾ ì›ìë£Œ ë¯¸ë¦¬ë³´ê¸°"):
    # ë¯¸ë¦¬ë³´ê¸°ìš© ë°ì´í„°í”„ë ˆì„: í™”ë©´ì— ë³´ì´ëŠ” ê²ƒê³¼ 'ì™„ì „íˆ ë™ì¼'í•˜ê²Œ êµ¬ì„±
    preview_df = (
        plot_df[["date", "src", "value_adj"]]
        .rename(columns={
            "date": "ë‚ ì§œ",
            "src": "ìë£Œì›",
            # í™”ë©´ í‘œê¸°ì™€ ë™ì¼í•œ ë¼ë²¨ ì‚¬ìš© (ì˜ˆ: 1880-01=0)
            "value_adj": f"í•´ìˆ˜ë©´({unit}, ì‹œì‘ì—°ì›”=0)"
        })
        .reset_index(drop=True)
    )

    # í™”ë©´ í‘œì‹œ
    st.dataframe(preview_df, use_container_width=True)

    # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (ì—‘ì…€ í˜¸í™˜ì„ ìœ„í•´ UTF-8 with BOM)
    csv_bytes = preview_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        label="ğŸ“¥ ìœ„ ë¯¸ë¦¬ë³´ê¸° ê·¸ëŒ€ë¡œ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv_bytes,
        file_name=f"gmsl_preview_{year_range[0]}-{year_range[1]}_{unit}_start0.csv",
        mime="text/csv",
        help="í˜„ì¬ 'ì›ìë£Œ ë¯¸ë¦¬ë³´ê¸°'ì— ë³´ì´ëŠ” í‘œë¥¼ ê·¸ëŒ€ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
    )


with st.expander("ğŸ“– ìš©ì–´ í’€ì´", expanded=False):
    st.markdown("""
- **ì¡°ìœ„ê³„ (Tide gauge)**  
  ë°”ë‹·ê°€ì— ì„¤ì¹˜ëœ ì¥ë¹„ë¡œ **í•´ìˆ˜ë©´ ë†’ì´**ë¥¼ ì‹œê°„ì— ë”°ë¼ ì¸¡ì •.  
  â†’ ì—°ì•ˆì— ì„¸ì›Œ ë‘” â€˜ìâ€™ì™€ ê°™ì€ ì—­í• .

- **ë³´ê°„ (Interpolation)**  
  ê´€ì¸¡ë˜ì§€ ì•Šì€ ì‹œê³µê°„ ê°’ì„ **ì£¼ë³€ ë°ì´í„°ë¡œ ì¶”ì •**í•´ ì±„ì›Œ ë„£ëŠ” ë°©ë²•.  
  â†’ 1ì›”Â·3ì›” ê°’ìœ¼ë¡œ 2ì›” ê°’ì„ ì¶”ì •í•˜ëŠ” ê²ƒê³¼ ê°™ìŒ.

- **GMSL (Global Mean Sea Level)**  
  ì§€êµ¬ ì „ì²´ ë°”ë‹¤ì˜ **í‰ê·  í•´ìˆ˜ë©´ ë†’ì´**.  
  â†’ ì§€ì—­ ì°¨ì´ë¥¼ ëª¨ë‘ í‰ê· ë‚¸ ê°’.

- **ì—°ì•ˆ ì¤‘ì‹¬ ìƒ˜í”Œë§ (Coastal sampling bias)**  
  ì¡°ìœ„ê³„ëŠ” ëŒ€ë¶€ë¶„ **í•­êµ¬Â·ì—°ì•ˆ**ì— ì„¤ì¹˜ â†’ **í•´ì–‘ ì „ì²´ í‰ê· ì— í¸í–¥**ì´ ìƒê¹€.

- **GIA (Glacial Isostatic Adjustment)**  
  ë¹™í•˜ê°€ ëˆŒë €ë˜ ë•…ì´ ë¹™í•˜ê°€ ë…¹ì€ ë’¤ **ì²œì²œíˆ ë‹¤ì‹œ ì†Ÿì•„ì˜¤ë¥´ëŠ” í˜„ìƒ**.  
  â†’ ìƒëŒ€ì  í•´ìˆ˜ë©´ ê°’ ë³´ì • í•„ìš”.

- **IB (Inverse Barometer correction)**  
  ê¸°ì•• ë³€í™”ì— ë”°ë¼ ë°”ë‹·ë¬¼ì´ ëˆŒë¦¬ê±°ë‚˜ ì˜¬ë¼ê°€ëŠ” í˜„ìƒ.  
  â†’ **ê¸°ì•• ì˜í–¥ ì œê±° ë³´ì •**.

- **TOPEX/Poseidon ì´í›„ ìœ„ì„± ê³ ë„ê³„ ê¸°ë°˜**  
  1992ë…„ ë°œì‚¬ëœ ìœ„ì„±ë¶€í„° **ë ˆì´ë”ë¡œ í•´ìˆ˜ë©´ê¹Œì§€ ê±°ë¦¬ ì¸¡ì •**.  
  â†’ ì „ ì§€êµ¬ ë°”ë‹¤ë¥¼ ê³ ë¥´ê²Œ ê´€ì¸¡ ê°€ëŠ¥.

- **ê¶¤ë„Â·ê³„ê¸° ë³´ì • (Orbital / Instrument correction)**  
  ìœ„ì„± ê¶¤ë„ í”ë“¤ë¦¼, ê³„ê¸° ì˜¤ì°¨ ë“±ì„ **ìˆ˜í•™ì ìœ¼ë¡œ ìˆ˜ì •**.  
  â†’ ì¤„ì ëˆˆê¸ˆì„ êµì •í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬.

- **ì°¸ì¡°ë©´ / ì§€ì˜¤ì´ë“œ ì²˜ë¦¬ (Reference frame / Geoid)**  
  í•´ìˆ˜ë©´ ë†’ì´ë¥¼ ì´ ë•Œ ê¸°ì¤€ì´ ë˜ëŠ” **í‰ë©´**ì´ë‚˜ **ì¤‘ë ¥í‰ë©´(ì§€ì˜¤ì´ë“œ)**.  
  â†’ â€œí•´ë°œâ€ ê¸°ì¤€ì„ ì •í•˜ëŠ” ê²ƒê³¼ ê°™ìŒ.

- **ê³„ì ˆì„±(ì—°ì£¼ê¸°) ì œê±° ì—¬ë¶€ (Seasonal / Annual cycle removal)**  
  í•´ìˆ˜ë©´ì€ ê³„ì ˆì— ë”°ë¼ ë³€ë™(ì—¬ë¦„â†‘, ê²¨ìš¸â†“).  
  â†’ ë¶„ì„í•  ë•Œ **ê³„ì ˆì„± í¬í•¨(keep)** ë˜ëŠ” **ì œê±°(free)** ì„ íƒ ê°€ëŠ¥.
""")


st.markdown("---")
st.caption("â“’ ë¯¸ë¦¼ë§ˆì´ìŠ¤í„°ê³  1í•™ë…„ 4ë°˜ 4ì¡° **ë§ˆìŒë°”ë‹¤ê±´ê°•ì¡°**")